{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Layers\n",
    "\n",
    "PyTorch is build around torch.nn, which have classes like `torch.nn.Module`, `torch.nn.Parameter`. Think of these as basic building blocks, or Lego blocks, that allow you to build complex structures. Layers, Models inherit from `torch.nn.Module`, which already defines a lot of useful methods and allows us to build our own blocks with as little as defining a `__init__()` and a `forward()` methods.\n",
    "\n",
    "For example, we can create a new layer with something as simple as:\n",
    "```python\n",
    "import torch\n",
    "class BasicLinear(torch.nn.Module): # Inherits from nn.Module. Almost everything in PyTorch is a nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = torch.nn.Parameter( # defines weights Parameter\n",
    "            data=torch.randn(1), # starts with random value\n",
    "            requires_grad=True # activate autograd, which allows us to track and update this parameter during backward pass\n",
    "        )\n",
    "        self.bias = torch.nn.Parameter( # defines bias Parameter, same as above but with different syntax\n",
    "            torch.randn(1, dtype=torch.float32), # explicitly sets the dtype to float32 (the default)\n",
    "            requires_grad=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor): # defines the way data will be transformed in the layer or block\n",
    "        # linear = xA^T + b\n",
    "        # A^T\n",
    "        weights = self.weights.t() # transposes weights tensor\n",
    "        # x\n",
    "        input_tensor = x\n",
    "        # b\n",
    "        bias = self.bias\n",
    "\n",
    "        # xA^T\n",
    "        mul = torch.matmul(input_tensor, weights) # calculates tensor dot product between transposed weights and input\n",
    "\n",
    "        # xA^T + b\n",
    "        output = mul + bias # adds offset (bias)\n",
    "\n",
    "        return output\n",
    "```\n",
    "In the forward method we follow the [implementation of the `nn.functional.linear` layer](https://pytorch.org/docs/stable/generated/torch.nn.functional.linear.html), which is implemented in C++. The [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) uses the ``nn.functional.linear` layer (written in C++) in its forward method. But at the end of the day, the transformation being made is:\n",
    "\n",
    "$$y=x A^{T}+b.$$\n",
    "\n",
    "In this notebook we are gonna see how we can create our own layers, and explore some of the predefined layers included with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
      "Requirement already satisfied: numpy==1.25.2 in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
      "Requirement already satisfied: pillow==9.4.0 in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
      "Requirement already satisfied: torchvision==0.18 in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
      "Torch version: 2.3.0+cu121\n",
      "Numpy version: 1.25.2\n",
      "PIL version: 9.4.0\n",
      "GPU enabled: False\n"
     ]
    }
   ],
   "source": [
    "# Ensures versions are correct\n",
    "! pip install torch==2.3.0 numpy==1.25.2 pillow==9.4.0 torchvision==0.18\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"PIL version: {PIL.__version__}\")\n",
    "print(f\"GPU enabled: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Building Blocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layers\n",
    "\n",
    "You can build your own layers using class definitions that inherit from torch.nn.Module. These can be used as building blocks for larger networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class BasicLinear(\n",
    "    torch.nn.Module\n",
    "):  # Inherits from nn.Module. Almost everything in PyTorch is a nn.Module\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super().__init__()\n",
    "        self.weights = torch.nn.Parameter(  # defines weights Parameter\n",
    "            data=torch.randn(\n",
    "                size=(output_features, input_features)\n",
    "            ),  # starts with random value\n",
    "            requires_grad=True,  # activate autograd, which allows us to track and update this parameter during backward pass\n",
    "        )\n",
    "        self.bias = torch.nn.Parameter(  # defines bias Parameter, same as above but with different syntax\n",
    "            torch.randn(\n",
    "                size=(output_features,), dtype=torch.float32\n",
    "            ),  # explicitly sets the dtype to float32 (the default)\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor\n",
    "    ) -> torch.Tensor:  # defines the way data will be transformed in the layer or block\n",
    "        # linear = xA^T + b\n",
    "        # A^T\n",
    "        weights = self.weights.t()  # transposes weights tensor\n",
    "        # x\n",
    "        input_tensor = x\n",
    "        # b\n",
    "        bias = self.bias\n",
    "\n",
    "        # xA^T\n",
    "        mul = torch.matmul(\n",
    "            input_tensor, weights\n",
    "        )  # calculates tensor dot product between transposed weights and input\n",
    "\n",
    "        # xA^T + b\n",
    "        output = mul + bias  # adds offset (bias)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry about the dimensions yet, we will understand it more later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicLinear()"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = BasicLinear(input_features=10, output_features=5)\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047,\n",
      "         -0.7521,  1.6487],\n",
      "        [-0.3925, -1.4036, -0.7279, -0.5594, -0.7688,  0.7624,  1.6423, -0.1596,\n",
      "         -0.4974,  0.4396],\n",
      "        [-0.7581,  1.0783,  0.8008,  1.6806,  1.2791,  1.2964,  0.6105,  1.3347,\n",
      "         -0.2316,  0.0418],\n",
      "        [-0.2516,  0.8599, -1.3847, -0.8712,  0.0780,  0.5258, -0.4880,  1.1914,\n",
      "         -0.8140, -0.7360],\n",
      "        [-0.8371, -0.9224, -0.0635,  0.6756, -0.0978,  1.8446, -1.1845,  1.3835,\n",
      "         -1.2024,  0.7078]], requires_grad=True) torch.Size([5, 10])\n",
      "Parameter containing:\n",
      "tensor([-0.5687,  1.2580, -1.5890, -1.1208,  0.8423], requires_grad=True) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for param in layer.parameters():\n",
    "    print(param, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3383,  1.6992,  0.0109, -0.3387, -1.3407, -0.5854,  0.5362,  0.5246,\n",
       "         -1.4692,  1.4332],\n",
       "        [ 0.7440, -0.4816, -1.0495,  0.6039, -1.7223, -0.8278, -0.4976,  0.4747,\n",
       "         -2.5095,  0.4880],\n",
       "        [ 0.7846,  0.0286,  0.6408,  0.5832,  0.2191,  0.5526, -0.1853,  0.7528,\n",
       "          0.4048,  0.1785],\n",
       "        [ 0.2649,  1.2732, -0.8905,  0.4098,  1.9312,  1.0119, -1.4364, -1.1299,\n",
       "         -0.1360,  1.6354],\n",
       "        [ 0.6547,  0.5760,  1.1415,  0.0186, -1.8058,  0.9254, -0.3753,  1.0331,\n",
       "         -0.6867,  0.6368]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(\n",
    "    size=(5, 10),\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.7495,  1.6639, -1.6199,  0.6273,  0.6853],\n",
       "        [-0.2628,  3.3308, -4.8425,  1.1273,  4.3891],\n",
       "        [-1.4089, -0.1772,  1.1424, -1.8549,  2.4110],\n",
       "        [ 4.9050, -2.3195,  1.0556, -0.2721,  2.9166],\n",
       "        [ 0.0959,  1.2854, -0.2936, -0.5371,  4.7370]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = layer(data)\n",
    "print(output.shape)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Networks and complex blocks\n",
    "\n",
    "Now that we have our custom layer, we can now use it to build more complex structures, like blocks or even entire networks.\n",
    "\n",
    "Let's start by defining a small neural network using our previously defined custom layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = BasicLinear(\n",
    "            input_features=10,  # expects input with shape (n, 10)\n",
    "            output_features=15,\n",
    "        )\n",
    "        self.layer2 = BasicLinear(\n",
    "            input_features=15,\n",
    "            output_features=20,  # should output with shape (n, 20)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        output_layer1 = self.layer1(x)\n",
    "        output_layer2 = self.layer2(output_layer1)\n",
    "        return output_layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.9269e+00,  1.4873e+00,  9.0072e-01, -2.1055e+00,  6.7842e-01,\n",
      "         -1.2345e+00, -4.3067e-02, -1.6047e+00, -7.5214e-01,  1.6487e+00],\n",
      "        [-3.9248e-01, -1.4036e+00, -7.2788e-01, -5.5943e-01, -7.6884e-01,\n",
      "          7.6245e-01,  1.6423e+00, -1.5960e-01, -4.9740e-01,  4.3959e-01],\n",
      "        [-7.5813e-01,  1.0783e+00,  8.0080e-01,  1.6806e+00,  1.2791e+00,\n",
      "          1.2964e+00,  6.1047e-01,  1.3347e+00, -2.3162e-01,  4.1759e-02],\n",
      "        [-2.5158e-01,  8.5986e-01, -1.3847e+00, -8.7124e-01, -2.2337e-01,\n",
      "          1.7174e+00,  3.1888e-01, -4.2452e-01,  3.0572e-01, -7.7459e-01],\n",
      "        [-1.5576e+00,  9.9564e-01, -8.7979e-01, -6.0114e-01, -1.2742e+00,\n",
      "          2.1228e+00, -1.2347e+00, -4.8791e-01, -9.1382e-01, -6.5814e-01],\n",
      "        [ 7.8024e-02,  5.2581e-01, -4.8799e-01,  1.1914e+00, -8.1401e-01,\n",
      "         -7.3599e-01, -1.4032e+00,  3.6004e-02, -6.3477e-02,  6.7561e-01],\n",
      "        [-9.7807e-02,  1.8446e+00, -1.1845e+00,  1.3835e+00,  1.4451e+00,\n",
      "          8.5641e-01,  2.2181e+00,  5.2317e-01,  3.4665e-01, -1.9733e-01],\n",
      "        [-1.0546e+00,  1.2780e+00, -1.7219e-01,  5.2379e-01,  5.6622e-02,\n",
      "          4.2630e-01,  5.7501e-01, -6.4172e-01, -2.2064e+00, -7.5080e-01],\n",
      "        [ 1.0868e-02, -3.3874e-01, -1.3407e+00, -5.8537e-01,  5.3619e-01,\n",
      "          5.2462e-01,  1.1412e+00,  5.1644e-02,  7.4395e-01, -4.8158e-01],\n",
      "        [-1.0495e+00,  6.0390e-01, -1.7223e+00, -8.2777e-01,  1.3347e+00,\n",
      "          4.8354e-01, -2.5095e+00,  4.8800e-01,  7.8459e-01,  2.8647e-02],\n",
      "        [ 6.4076e-01,  5.8325e-01,  1.0669e+00, -4.5015e-01, -1.8527e-01,\n",
      "          7.5276e-01,  4.0476e-01,  1.7847e-01,  2.6491e-01,  1.2732e+00],\n",
      "        [-1.3109e-03, -3.0360e-01, -1.4570e+00, -1.0234e-01, -5.9915e-01,\n",
      "          4.7706e-01,  7.2618e-01,  9.1152e-02, -3.8907e-01,  5.2792e-01],\n",
      "        [-1.2685e-02,  2.4084e-01,  1.3254e-01,  7.6424e-01,  1.0950e+00,\n",
      "          3.3989e-01,  7.1997e-01,  4.1141e-01,  1.9312e+00,  1.0119e+00],\n",
      "        [-1.4364e+00, -1.1299e+00, -1.3603e-01,  1.6354e+00, -7.3280e-01,\n",
      "          1.0430e-01,  1.0414e+00, -3.9973e-01, -2.2933e+00,  4.9756e-01],\n",
      "        [-4.2572e-01, -1.3371e+00, -1.1955e+00,  8.1234e-01, -3.0628e-01,\n",
      "         -3.3016e-01, -9.8080e-01,  1.9473e-01, -1.6535e+00,  6.8142e-01]],\n",
      "       requires_grad=True) torch.Size([15, 10])\n",
      "Parameter containing:\n",
      "tensor([ 1.0448,  1.8319,  0.7205, -0.1121, -0.0309, -0.1503,  1.8928,  1.3067,\n",
      "        -0.0662, -0.4235, -2.3768,  0.0641, -0.3435,  1.2287, -0.2754],\n",
      "       requires_grad=True) torch.Size([15])\n",
      "Parameter containing:\n",
      "tensor([[ 0.4788,  1.3537, -0.1593, -0.4249,  0.9442, -0.1849,  1.0608,  0.2083,\n",
      "          1.3065,  0.4598,  0.2618, -0.7599, -2.0461, -1.5295,  0.4049],\n",
      "        [ 0.6319, -0.1483, -2.3184,  1.3032,  0.4879,  1.1340, -0.3556,  0.3618,\n",
      "          1.9993,  1.0350,  1.6896,  0.0213, -0.8293, -1.0809, -0.7839],\n",
      "        [ 0.5071,  0.0821,  0.3999,  1.9892, -0.4611, -0.0639, -1.3667,  0.3298,\n",
      "         -0.9827,  0.3018,  0.1932,  0.4097, -1.5754,  2.2508,  1.0012],\n",
      "        [ 1.3642,  0.6333,  0.4050, -0.3532,  1.4639,  0.1729,  1.0514,  0.0075,\n",
      "         -0.0774,  0.6427,  0.5742,  0.5058,  0.2225, -0.9143,  1.4840],\n",
      "        [-0.9109, -0.5291, -0.8051,  0.5158, -0.1250,  0.7821,  0.5635,  1.8582,\n",
      "          1.0441, -0.8638,  0.8351, -0.3157, -1.9776,  0.0179, -1.4129],\n",
      "        [-1.8791, -0.1798,  0.7904,  0.5239, -0.2694,  1.7093,  0.0579,  0.8637,\n",
      "         -0.5890, -1.0340, -0.2179,  0.7987,  0.9105,  0.2691, -0.0366],\n",
      "        [-0.4808,  0.3163,  0.3866,  0.7337,  0.2510,  0.0770, -0.7150, -0.0476,\n",
      "          0.5230,  0.9717, -0.2779, -0.6116, -0.5572, -0.9683,  1.3433],\n",
      "        [ 0.7133,  0.3463, -0.5402,  0.8569, -0.6721,  1.0682, -0.2527, -2.3065,\n",
      "         -1.2869,  0.2137, -1.2351,  1.8592,  0.0561,  0.7694,  2.5574],\n",
      "        [ 1.2049, -0.9825,  0.3040,  0.9339, -1.9726, -1.4120,  1.7361,  1.8350,\n",
      "         -0.0047,  0.0795, -0.4560, -0.0619, -0.2222, -1.2470, -0.4862],\n",
      "        [-0.3360, -0.5871,  0.0827,  0.1858, -0.9698,  1.8932,  0.4447,  0.1364,\n",
      "          0.3088, -0.4875,  0.0501,  0.3273,  0.1292,  2.8520, -0.7436],\n",
      "        [ 0.1954, -1.3350, -0.5730, -0.3303, -0.3071, -0.7155,  0.0762, -0.2127,\n",
      "         -0.5663,  0.3989, -0.5513,  1.9890,  0.8479, -0.6953,  0.3056],\n",
      "        [ 0.2909,  0.4085, -1.2609, -0.3947,  1.8868,  0.1787, -0.0385, -0.0869,\n",
      "         -1.1803,  1.5460,  0.5448, -0.2040,  0.6854, -1.3351,  1.6516],\n",
      "        [ 1.9810, -0.1048,  0.4903, -0.4375, -0.4901, -0.3595, -0.0589, -0.4809,\n",
      "          0.9933,  0.2695, -1.8316,  0.3570, -0.6013, -0.0996, -1.2311],\n",
      "        [ 0.8657, -1.4236, -0.6961, -0.3182,  1.2154, -0.8076,  0.8244,  1.4862,\n",
      "         -1.4091, -0.3639, -0.0993,  0.3105,  0.3715,  0.1527, -0.0388],\n",
      "        [-0.2130, -0.5904,  0.4668,  0.3956, -2.7107, -0.6130, -0.1932,  0.7768,\n",
      "          0.7204, -1.3358,  0.8108,  0.7546,  0.4145,  1.3664, -0.0462],\n",
      "        [-1.6457, -0.6960,  0.2359,  0.8281,  0.1104, -0.3016, -1.4033, -0.5151,\n",
      "          0.6903,  2.0403, -1.2979,  0.6161, -0.2508,  0.8211, -0.0675],\n",
      "        [ 0.6686,  1.0430,  0.7204,  1.4701, -1.1215,  2.1004,  2.3664, -0.6323,\n",
      "          0.7607, -0.7463, -0.5824,  1.3500,  0.2778,  0.0916,  1.5164],\n",
      "        [-0.4389, -0.6421,  0.8266, -0.3646,  0.0872,  1.1225,  0.0980,  1.1226,\n",
      "          1.2491, -1.2620,  1.0861, -0.8786,  2.5487,  0.6594,  0.6702],\n",
      "        [-0.1260, -0.6126, -1.3062, -1.4040,  0.0953, -0.3659,  0.1961, -1.5174,\n",
      "          0.2059,  0.2722,  2.0487, -1.0880, -2.0479,  1.0006,  0.6495],\n",
      "        [ 0.0950, -0.7526, -0.6472, -1.2823,  1.9653, -1.1766,  1.1889,  0.7096,\n",
      "          0.8198,  0.6214,  0.4232, -0.3389,  0.5180, -1.7459,  1.0964]],\n",
      "       requires_grad=True) torch.Size([20, 15])\n",
      "Parameter containing:\n",
      "tensor([-0.6103,  0.1632,  1.5102,  0.2123, -1.4575, -0.9712,  0.2415, -1.1612,\n",
      "         0.1451,  1.6612,  0.5439, -0.5214,  1.2208, -0.6076, -0.0453, -0.3573,\n",
      "        -0.5714, -1.4078, -0.3764,  0.9874], requires_grad=True) torch.Size([20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (layer1): BasicLinear()\n",
       "  (layer2): BasicLinear()\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNet()\n",
    "for param in model.parameters():\n",
    "    print(param, param.shape)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1678,  1.6433,  0.5163,  1.6060, -0.9815,  0.5361,  0.9226,  0.4872,\n",
       "         -0.9770, -0.0336],\n",
       "        [-0.7983, -0.2648, -0.1666,  0.2518,  1.2571,  1.2173,  0.3034, -0.4501,\n",
       "         -0.1739,  0.0299],\n",
       "        [-0.0140, -0.0102,  0.2337,  1.4083, -0.1743,  0.6092,  0.2254, -0.2793,\n",
       "          0.6702,  0.1188],\n",
       "        [-0.6119,  0.6026, -0.4403,  2.1848,  0.5258,  1.6828,  0.0967,  0.2571,\n",
       "          0.4728,  0.3640],\n",
       "        [-0.2812, -1.0375, -0.4976, -0.1823, -0.2120,  0.8162,  0.8982, -0.1539,\n",
       "         -0.5682, -0.0868]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(\n",
    "    size=(5, 10),\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ -3.1472, -29.9071,   8.7836,   3.4756,  15.5517,  23.3254, -13.9781,\n",
       "          -16.4666,  11.5610,  26.2780, -12.2689, -20.9407,  -4.5172,  15.7965,\n",
       "           12.2754, -17.7336,  22.0269,  18.4722, -17.1346,  -5.1706],\n",
       "         [  4.1676, -14.8314,   3.4650,   6.1778,  -1.4382,   4.5206,   0.0622,\n",
       "           -9.7195,  10.3086,   6.7338,  -5.2735,  -7.4848,   3.2957,   2.4533,\n",
       "            1.2410,   4.0957,  13.7237,   4.0555, -17.8447,   4.4652],\n",
       "         [ -6.3661, -20.4782,  -3.6122,  -2.0613,  -1.2317,  14.0660,  -6.6215,\n",
       "           -4.6526,   2.7820,  14.8900,  -4.2800, -12.9097,  -0.8511,  -0.8365,\n",
       "            6.9943,  -4.1665,  15.3426,  11.1509, -13.1717,  -4.7658],\n",
       "         [ -5.4236, -28.9025,  -4.7860,   6.7843,  -1.0098,  27.0510,  -2.1414,\n",
       "          -10.6474,   5.8732,  19.9806,  -5.5848, -10.5738,  -9.7988,   1.8492,\n",
       "            2.7274,   5.9610,  28.6326,  21.9810, -26.0162,   4.0024],\n",
       "         [  5.9202, -11.4263,  14.1665,   0.8641,   3.0614,   4.1301,  -1.4829,\n",
       "            3.4788,  -2.9597,   9.9622,  -8.0175, -10.9831,   1.3440,  -3.1530,\n",
       "            6.8455,   1.7525,  15.6745,  -3.3727,  -9.1926,  -5.7175]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([5, 20]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(data)\n",
    "output, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! The output matches the expected shape\n",
    "\n",
    "Now let's try to implement a block and use it to build a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class SimpleResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.fc1 = torch.nn.Linear(in_features=in_features, out_features=out_features)\n",
    "        self.fc2 = torch.nn.Linear(in_features=out_features, out_features=out_features)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.downsample = None\n",
    "\n",
    "        # allows us to ensure the identity have the same shape\n",
    "        # as the output\n",
    "        if in_features != out_features:\n",
    "            self.downsample = torch.nn.Linear(\n",
    "                in_features=in_features, out_features=out_features\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        identity = x\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "        out += identity\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleResidualBlock(\n",
       "  (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (downsample): Linear(in_features=10, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = SimpleResidualBlock(in_features=10, out_features=20)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3382,  0.4742, -2.2940,  0.7744, -0.5453, -2.1582, -1.6608, -0.6637,\n",
       "         -0.2670,  0.2584],\n",
       "        [ 0.7758, -0.1000, -0.5615, -0.5949,  1.2687,  1.2904,  0.6930,  1.1980,\n",
       "          1.3964, -0.7150],\n",
       "        [ 1.4109, -1.3144, -1.3162, -1.2524, -1.6489, -0.2800, -1.2407,  0.7410,\n",
       "          0.7378, -0.8505],\n",
       "        [ 0.0361,  1.3407,  0.9860,  0.1132, -0.4233, -1.9508,  1.8619, -1.0779,\n",
       "          0.8849, -0.8342],\n",
       "        [ 1.0301, -0.8681,  0.2418,  1.3824,  1.1285, -1.2123,  2.6024, -0.0957,\n",
       "         -0.0811,  1.2587]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(\n",
    "    size=(5, 10),\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4739, -1.9992,  1.0667, -1.4696,  0.4428, -0.8600,  0.2264, -1.3219,\n",
       "          0.2252,  0.6634,  0.0587,  0.2295,  0.3469,  0.8965,  0.0336,  1.0089,\n",
       "          1.1820, -0.3060,  0.1728,  0.5566],\n",
       "        [-0.7263, -0.6590,  0.3153, -1.6100,  0.0537, -0.3231, -0.4889,  0.3054,\n",
       "         -0.8394,  0.5122, -1.0412, -0.0720, -1.4181,  0.3687, -1.0107, -0.8467,\n",
       "          0.2860, -0.1833,  0.4961,  0.4846],\n",
       "        [ 0.5692, -1.1106, -0.5407, -1.6185,  1.0318, -1.6888,  0.5215, -1.2556,\n",
       "         -1.0459,  0.9259, -1.0388, -0.6898, -0.3632,  1.4558, -0.7626, -0.7203,\n",
       "          0.7005, -1.4810,  0.3062,  1.5153],\n",
       "        [ 0.3583, -1.3263, -0.3940, -1.0462, -1.2971, -0.6285, -0.0323,  0.1265,\n",
       "          0.2430, -1.2454,  0.8915, -0.8012,  0.0533,  0.3077, -0.1811,  0.4346,\n",
       "          0.8679, -0.7155,  1.0592, -0.4466],\n",
       "        [ 0.2313, -0.1927, -0.9310, -1.5572, -1.4468,  0.2535,  0.6084, -0.5691,\n",
       "         -0.3841,  0.3957,  1.6790,  1.0765, -0.0834, -0.4265,  0.9681,  0.5116,\n",
       "          0.3979,  0.3903, -0.2494, -0.6807]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = block(data)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our block, let's use it to build a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.block1 = SimpleResidualBlock(\n",
    "            in_features=in_features, out_features=out_features\n",
    "        )\n",
    "        self.block2 = SimpleResidualBlock(\n",
    "            in_features=out_features, out_features=out_features\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (block1): SimpleResidualBlock(\n",
       "    (fc1): Linear(in_features=10, out_features=50, bias=True)\n",
       "    (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (downsample): Linear(in_features=10, out_features=50, bias=True)\n",
       "  )\n",
       "  (block2): SimpleResidualBlock(\n",
       "    (fc1): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNet(in_features=10, out_features=50)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2182,  0.8823,  0.5390,  1.3357,  0.8349, -1.0390, -0.4415, -0.4136,\n",
       "          0.6149,  0.5247],\n",
       "        [ 0.1156,  0.9289, -1.1753,  1.4462,  0.2890, -0.5746,  0.4203,  0.3187,\n",
       "         -0.1949, -0.7710],\n",
       "        [-1.0754, -0.6555, -0.5378, -0.3390,  1.3493, -1.5745, -0.5291,  2.2761,\n",
       "          0.2758,  0.4236],\n",
       "        [-1.7807, -0.2473,  1.3181,  1.8177,  1.5550,  1.1142, -0.2878, -1.0536,\n",
       "         -1.5974, -0.1525],\n",
       "        [ 0.2308,  1.0065,  0.1740,  1.5454, -0.8084,  1.7691,  0.1786,  0.5163,\n",
       "         -0.4629, -0.6336]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(\n",
    "    size=(5, 10),\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(data)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we can use and add any class that inherits from the `torch.nn.Module` as lego blocks for larger blocks or networks. This allows for a lot of flexibility when designing networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "In this section we will explore some of the layers available in the pytorch ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "This is one of the most basic layers, which is a fully connected layer that applies a linear transformation to the input. It is used, for example, as the final layer of Convolutional Neural Networks to output the classes probabilities.\n",
    "\n",
    "The output is given by:\n",
    "$$ y = x\\cdot{A^T} + b $$\n",
    "\n",
    "Where:\n",
    "- $x$ is the input of the layer\n",
    "- $A$ is the weights matrix, which in this case is transposed ($A^T$)\n",
    "- $b$ is the bias term\n",
    "\n",
    "In other words, the linear layer is a dot product of the input tensor and the weights tensor, plus the bias term to offset the weights and inputs\n",
    "\n",
    "**Documentation**\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.linear.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7910, -0.6975,  0.4384,  0.7299,  1.0319],\n",
       "        [-0.2977,  0.5749, -0.3397,  0.9044,  1.0887],\n",
       "        [ 0.3056, -0.6722, -0.0591,  0.5983, -0.2779],\n",
       "        [ 0.1036, -0.0570,  0.0212,  0.9951,  0.7813],\n",
       "        [ 0.5157,  0.1053, -1.0661,  1.8080,  0.9811]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "linear = torch.nn.Linear(\n",
    "    in_features=10,  # in_features = matches inner dimension of input\n",
    "    out_features=5,\n",
    ")  # out_features = describes outer value of output\n",
    "# this layer expects a (n, 10) input and will output a (n, 5). See below.\n",
    "\n",
    "x = torch.randn(\n",
    "    size=(5, 10),\n",
    ")\n",
    "output = linear(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2418,  0.2625, -0.0741,  0.2905, -0.0693,  0.0638, -0.1540,  0.1857,\n",
      "          0.2788, -0.2320],\n",
      "        [ 0.2749,  0.0592,  0.2336,  0.0428,  0.1525, -0.0446,  0.2438,  0.0467,\n",
      "         -0.1476,  0.0806],\n",
      "        [-0.1457, -0.0371, -0.1284,  0.2098, -0.2496, -0.1458, -0.0893, -0.1901,\n",
      "          0.0298, -0.3123],\n",
      "        [ 0.2856, -0.2686,  0.2441,  0.0526, -0.1027,  0.1954,  0.0493,  0.2555,\n",
      "          0.0346, -0.0997],\n",
      "        [ 0.0850, -0.0858,  0.1331,  0.2823,  0.1828, -0.1382,  0.1825,  0.0566,\n",
      "          0.1606, -0.1927]], requires_grad=True) torch.Size([5, 10])\n",
      "Parameter containing:\n",
      "tensor([-0.3130, -0.1222, -0.2426,  0.2595,  0.0911], requires_grad=True) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for param in linear.parameters():\n",
    "    print(param, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with different dimensions this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4960, -0.2588,  0.8303],\n",
       "        [ 0.1278, -0.1437,  0.2210],\n",
       "        [-0.5078, -0.1890, -0.4775],\n",
       "        [-1.1359, -0.5379, -0.2967],\n",
       "        [-0.4981, -0.2988, -0.2436],\n",
       "        [ 0.3976,  0.0798,  0.2285],\n",
       "        [-0.1890, -0.3357,  0.3579],\n",
       "        [-0.3978, -0.8552,  0.5626],\n",
       "        [-0.0656, -0.3253,  0.0333]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "linear = torch.nn.Linear(\n",
    "    in_features=7,  # in_features = matches inner dimension of input\n",
    "    out_features=3,\n",
    ")  # out_features = describes outer value\n",
    "x = torch.randn(\n",
    "    size=(9, 7),\n",
    ")\n",
    "output = linear(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 3])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2890,  0.3137, -0.0885,  0.3472, -0.0828,  0.0763, -0.1840],\n",
      "        [ 0.2220,  0.3332, -0.2773,  0.3285,  0.0707,  0.2792,  0.0512],\n",
      "        [ 0.1822, -0.0534,  0.2914,  0.0559, -0.1764,  0.0963, -0.1741]],\n",
      "       requires_grad=True) torch.Size([3, 7])\n",
      "Parameter containing:\n",
      "tensor([-0.0443, -0.1535,  0.2507], requires_grad=True) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for param in linear.parameters():\n",
    "    print(param, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapes\n",
    "\n",
    "Let's try to understand the shapes we defined.\n",
    "\n",
    "When creating the layer, we defined 2 parameters:\n",
    "- in_features\n",
    "- out_features\n",
    "\n",
    "The figure below explains how the shapes of the inputs and outputs are obtained\n",
    "\n",
    "<img src=\"../assets/layers/linear layer.png\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilinear\n",
    "\n",
    "The bilinear layer correlates 2 tensors.\n",
    "\n",
    "The output is given by:\n",
    "$$ y=x_{1}^{T}A x_{2}+b $$\n",
    "\n",
    "Where:\n",
    "- $x_1$ is the input tensor 1 of the layer, in this case transposed $(x_1^T)$\n",
    "- $x_2$ is the input tensor 2 of the layer\n",
    "- $A$ is the weights matrix\n",
    "- $b$ is the bias term\n",
    "\n",
    "One thing to note is that *matrix dot products are associative* which means that\n",
    "\n",
    "$$(x_{1}^{T}A) x_{2} = x_{1}^{T} (Ax_{2})$$\n",
    "\n",
    "**Documentation**\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Bilinear.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.bilinear.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2347, -1.5629,  1.7704, -0.0386,  0.5411,  0.5182, -0.6519],\n",
       "         [-0.0134, -1.7769,  0.0528, -1.5926,  1.6024, -2.5127,  0.5474],\n",
       "         [-0.3513, -3.4169, -0.6370, -6.0918,  1.4707,  1.2421, -5.9193],\n",
       "         [ 0.6226, -1.2700,  1.2536, -0.3394, -1.7838,  1.3753, -5.1026],\n",
       "         [-1.1170, -1.3467, -1.2738, -2.9952, -2.9509,  0.4995,  1.9577]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([5, 7]))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "bilinear = torch.nn.Bilinear(\n",
    "    in1_features=10,  # in_features = matches inner dimension of input\n",
    "    in2_features=20,\n",
    "    out_features=7,  # out_features = describes outer value of output\n",
    ")\n",
    "# this layer expects a (n, 10) input and will output a (n, 20). See below.\n",
    "\n",
    "x1 = torch.randn(\n",
    "    size=(5, 10),\n",
    ")\n",
    "x2 = torch.randn(\n",
    "    size=(5, 20),\n",
    ")\n",
    "output = bilinear(x1, x2)\n",
    "output, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 10, 20])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the weights tensor is 3d. (out_features, in1_features)\n",
    "bilinear.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilinear.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually computing for 2d tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " tensor([[ 0.2347, -1.5629,  1.7704, -0.0386,  0.5411,  0.5182, -0.6519],\n",
       "         [-0.0134, -1.7769,  0.0528, -1.5926,  1.6024, -2.5127,  0.5474],\n",
       "         [-0.3513, -3.4169, -0.6370, -6.0918,  1.4707,  1.2421, -5.9193],\n",
       "         [ 0.6226, -1.2700,  1.2536, -0.3394, -1.7838,  1.3753, -5.1026],\n",
       "         [-1.1170, -1.3467, -1.2738, -2.9952, -2.9509,  0.4995,  1.9577]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[ 0.2347, -1.5629,  1.7704, -0.0386,  0.5411,  0.5182, -0.6519],\n",
       "         [-0.0134, -1.7769,  0.0528, -1.5926,  1.6024, -2.5127,  0.5474],\n",
       "         [-0.3513, -3.4169, -0.6370, -6.0918,  1.4707,  1.2421, -5.9193],\n",
       "         [ 0.6226, -1.2700,  1.2536, -0.3394, -1.7838,  1.3753, -5.1026],\n",
       "         [-1.1170, -1.3467, -1.2738, -2.9952, -2.9509,  0.4995,  1.9577]],\n",
       "        grad_fn=<CopySlices>))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract learnable weights\n",
    "A = bilinear.weight  # Shape: (out_features, in1_features, in2_features)\n",
    "b = bilinear.bias  # Shape: (out_features), if bias=True\n",
    "\n",
    "# Manual computation of the bilinear transformation\n",
    "# Initialize output matrix\n",
    "output_manual = torch.zeros((5, 7))  # batch, out_features\n",
    "\n",
    "# Compute y = x1^T * A * x2 + b for each batch and output feature\n",
    "for i in range(5):  # Iterate over each sample in the batch\n",
    "    for o in range(7):  # Iterate over each output feature\n",
    "        temp_result = (\n",
    "            x1[i].T @ A[o] @ x2[i]\n",
    "        )  # Compute dot product for specific batch element and output feature\n",
    "        # x1[i].T = transpose of each row, which is a vector in this case\n",
    "        # a vector of len = 7\n",
    "\n",
    "        # A[o] = 2d matrice. Each matrice corresponds to a row in the 3d tensor\n",
    "        # a 2d tensor of shape in1_feature (10) and in2_feature (20)\n",
    "        # x2[i] = each row in x2, which is a vector in this case\n",
    "        # a vector of len = 20\n",
    "\n",
    "        # temp result will be a scalar\n",
    "        temp_result += b[o]  # Add bias for the specific output feature\n",
    "\n",
    "        output_manual[i, o] = temp_result  # Store the result in the output tensor\n",
    "\n",
    "# Automatic computation with PyTorch\n",
    "output_auto = bilinear(x1, x2)\n",
    "\n",
    "# Comparing manual and automatic computations\n",
    "are_close = torch.allclose(output_manual, output_auto)\n",
    "are_close, output_auto, output_manual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 3D Tensors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 7])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "bilinear = torch.nn.Bilinear(\n",
    "    in1_features=10,  # in_features = matches inner dimension of input\n",
    "    in2_features=20,\n",
    "    out_features=7,  # out_features = describes outer value of output\n",
    ")\n",
    "# this layer expects a (n, 10) input and will output a (n, 20). See below.\n",
    "\n",
    "x1 = torch.randn(\n",
    "    size=(5, 15, 10),\n",
    ")\n",
    "x2 = torch.randn(\n",
    "    size=(5, 15, 20),\n",
    ")\n",
    "output = bilinear(x1, x2)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 10, 20])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilinear.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapes\n",
    "\n",
    "Let's try to understand the shapes we defined.\n",
    "\n",
    "When creating the layer, we defined 2 parameters:\n",
    "- in_features\n",
    "- out_features\n",
    "\n",
    "The figure below explains how the shapes of the inputs and outputs are obtained\n",
    "\n",
    "<img src=\"../assets/layers/Bilinear layer.png\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1d\n",
    "\n",
    "This applies a 1d convolution. This can be used for 1 dimensional data, like sequences.\n",
    "\n",
    "Considering a input of size $(N, C_{in}, L)$ that outputs a tensor with shape $(N, C_{out}, L_{out})$, the output is given by:\n",
    "$$ {\\mathrm{out}}(N_{i},C_{\\mathrm{out}_{j}})={\\mathrm{bias}}(C_{\\mathrm{out}_{j}})+\\sum_{k=0}^{C_{\\mathrm{in}}-1}{\\mathrm{weight}}(C_{\\mathrm{out}_{j}},k)\\star{\\mathrm{input}}(N_{i},k) $$\n",
    "\n",
    "Where:\n",
    "- $N$ is the batch size\n",
    "- $C$ is the number of channels\n",
    "- $L$ is the signal sequence\n",
    "- $\\star$ is the valid [cross correlation](https://en.wikipedia.org/wiki/Cross-correlation) operator\n",
    "\n",
    "In other words, the linear layer is a dot product of the input tensor and the weights tensor, plus the bias term to offset the weights and inputs\n",
    "\n",
    "Inputs:\n",
    "- *in_channels* is the number of channels in the input\n",
    "- *out_channels* is the number of channels produced\n",
    "- *kernel_size* is the size of the convolutional kernel \n",
    "\n",
    "To compute the expected size of $L_{out}$ you can use the following equation:\n",
    "$$ \n",
    "L_{out} = \\left\\lfloor \\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation} \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1 \\right\\rfloor \n",
    "$$\n",
    "\n",
    "or the following code:\n",
    "```python\n",
    "L_out = ((L_in + 2 * padding - dilation * (kernel_size - 1) - 1) // stride) + 1\n",
    "```\n",
    "\n",
    "Variables:\n",
    "- *weight*: The learnable weight with shape $\\left(\\mathrm{out\\_channels},\\,\\frac{\\mathrm{in\\_channels}}{\\mathrm{groups}},\\,\\mathrm{kernel\\_size}\\right)$\n",
    "- *bias*: The learnable bias with shape $(\\mathrm{out\\_channels})$\n",
    "\n",
    "\n",
    "**Documentation**\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.conv1d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "conv1d = torch.nn.Conv1d(\n",
    "    in_channels=3,\n",
    "    out_channels=5,\n",
    "    kernel_size=4,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    dilation=1,\n",
    "    groups=1,\n",
    ")\n",
    "x = torch.randn((6, 3, 8))\n",
    "output = conv1d(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3, 2])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with groups = 1, shape is:\n",
    "# (out_channels, in_channels, kernel_size)\n",
    "conv1d.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape is (out_channels)\n",
    "conv1d.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating/estimating $L_{out}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated L_out: 25\n",
      "conv1d L_out: 25\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def calculate_L_out(conv1d, x):\n",
    "    \"\"\"\n",
    "    Calculate the length of the output feature map (L_out) for a 1D convolution.\n",
    "\n",
    "    Parameters:\n",
    "        conv1d (torch.nn.Conv1d): The Conv1d layer.\n",
    "        x (Tensor): Input tensor with shape (batch_size, channels, L_in).\n",
    "\n",
    "    Returns:\n",
    "        int: Length of the output feature map (L_out).\n",
    "    \"\"\"\n",
    "    # Get the attributes from the convolution layer\n",
    "    kernel_size = conv1d.kernel_size[0]\n",
    "    stride = conv1d.stride[0]\n",
    "    padding = conv1d.padding[0]\n",
    "    dilation = conv1d.dilation[0]\n",
    "\n",
    "    # Calculate L_in using the input tensor shape\n",
    "    L_in = x.size(-1)  # The length of the input feature map (L_in)\n",
    "\n",
    "    # Compute L_out using the formula\n",
    "    L_out = ((L_in + 2 * padding - dilation * (kernel_size - 1) - 1) // stride) + 1\n",
    "\n",
    "    return int(L_out)\n",
    "\n",
    "\n",
    "conv1d = torch.nn.Conv1d(\n",
    "    in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1, dilation=1\n",
    ")\n",
    "\n",
    "# Create an input tensor with shape (batch_size, channels, L_in)\n",
    "batch_size = 4\n",
    "L_in = 50\n",
    "x = torch.randn(batch_size, 1, L_in)\n",
    "\n",
    "# Calculate L_out\n",
    "L_out = calculate_L_out(conv1d, x)\n",
    "print(f\"calculated L_out: {L_out}\")\n",
    "print(f\"conv1d L_out: {conv1d(x).shape[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8])\n",
      "torch.Size([7, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "conv1d = torch.nn.Conv1d(\n",
    "    in_channels=3,\n",
    "    out_channels=7,\n",
    "    kernel_size=2,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    dilation=1,\n",
    "    groups=1,\n",
    ")\n",
    "\n",
    "x = torch.randn((6, 3, 8))\n",
    "output = conv1d(x)\n",
    "print(x[0].shape)\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3, 2])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxPool\n",
    "\n",
    "This is one of the most basic layers, which is a fully connected layer that applies a linear transformation to the input. It is used, for example, as the final layer of Convolutional Neural Networks to output the classes probabilities.\n",
    "\n",
    "The output is given by:\n",
    "$$ y = x\\cdot{A^T} + b $$\n",
    "\n",
    "Where:\n",
    "- $x$ is the input of the layer\n",
    "- $A$ is the weights matrix, which in this case is transposed ($A^T$)\n",
    "- $b$ is the bias term\n",
    "\n",
    "In other words, the linear layer is a dot product of the input tensor and the weights tensor, plus the bias term to offset the weights and inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
