{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Layers\n",
    "\n",
    "PyTorch is build around torch.nn, which have classes like `torch.nn.Module`, `torch.nn.Parameter`. Think of these as basic building blocks, or Lego blocks, that allow you to build complex structures. Layers, Models inherit from `torch.nn.Module`, which already defines a lot of useful methods and allows us to build our own blocks with as little as defining a `__init__()` and a `forward()` methods.\n",
    "\n",
    "For example, we can create a new layer with something as simple as:\n",
    "```python\n",
    "import torch\n",
    "class BasicLinear(torch.nn.Module): # Inherits from nn.Module. Almost everything in PyTorch is a nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = torch.nn.Parameter( # defines weights Parameter\n",
    "            data=torch.randn(1), # starts with random value\n",
    "            requires_grad=True # activate autograd, which allows us to track and update this parameter during backward pass\n",
    "        )\n",
    "        self.bias = torch.nn.Parameter( # defines bias Parameter, same as above but with different syntax\n",
    "            torch.randn(1, dtype=torch.float32), # explicitly sets the dtype to float32 (the default)\n",
    "            requires_grad=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor): # defines the way data will be transformed in the layer or block\n",
    "        # linear = xA^T + b\n",
    "        # A^T\n",
    "        weights = self.weights.t() # transposes weights tensor\n",
    "        # x\n",
    "        input_tensor = x\n",
    "        # b\n",
    "        bias = self.bias\n",
    "\n",
    "        # xA^T\n",
    "        mul = torch.matmul(input_tensor, weights) # calculates tensor dot product between transposed weights and input\n",
    "\n",
    "        # xA^T + b\n",
    "        output = mul + bias # adds offset (bias)\n",
    "\n",
    "        return output\n",
    "```\n",
    "In the forward method we follow the [implementation of the `nn.functional.linear` layer](https://pytorch.org/docs/stable/generated/torch.nn.functional.linear.html), which is implemented in C++. The [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) uses the ``nn.functional.linear` layer (written in C++) in its forward method. But at the end of the day, the transformation being made is:\n",
    "\n",
    "$$y=x A^{T}+b.$$\n",
    "\n",
    "In this notebook we are gonna see how we can create our own layers, and explore some of the predefined layers included with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
      "Requirement already satisfied: numpy==1.25.2 in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
      "Requirement already satisfied: pillow==9.4.0 in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
      "Requirement already satisfied: torchvision==0.18 in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
      "Torch version: 2.3.0+cu121\n",
      "Numpy version: 1.25.2\n",
      "PIL version: 9.4.0\n",
      "GPU enabled: True\n"
     ]
    }
   ],
   "source": [
    "# Ensures versions are correct\n",
    "! pip install torch==2.3.0 numpy==1.25.2 pillow==9.4.0 torchvision==0.18\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"PIL version: {PIL.__version__}\")\n",
    "print(f\"GPU enabled: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Building Blocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layers\n",
    "\n",
    "You can build your own layers using class definitions that inherit from torch.nn.Module. These can be used as building blocks for larger networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "class BasicLinear(torch.nn.Module): # Inherits from nn.Module. Almost everything in PyTorch is a nn.Module\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super().__init__()\n",
    "        self.weights = torch.nn.Parameter( # defines weights Parameter\n",
    "            data=torch.randn(size=(output_features, input_features)), # starts with random value\n",
    "            requires_grad=True # activate autograd, which allows us to track and update this parameter during backward pass\n",
    "        )\n",
    "        self.bias = torch.nn.Parameter( # defines bias Parameter, same as above but with different syntax\n",
    "            torch.randn(size=(output_features,), dtype=torch.float32), # explicitly sets the dtype to float32 (the default)\n",
    "            requires_grad=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # defines the way data will be transformed in the layer or block\n",
    "        # linear = xA^T + b\n",
    "        # A^T\n",
    "        weights = self.weights.t() # transposes weights tensor\n",
    "        # x\n",
    "        input_tensor = x\n",
    "        # b\n",
    "        bias = self.bias\n",
    "\n",
    "        # xA^T\n",
    "        mul = torch.matmul(input_tensor, weights) # calculates tensor dot product between transposed weights and input\n",
    "\n",
    "        # xA^T + b\n",
    "        output = mul + bias # adds offset (bias)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry about the dimensions yet, we will understand it more later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicLinear()"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = BasicLinear(input_features=10, output_features=5)\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047,\n",
      "         -0.7521,  1.6487],\n",
      "        [-0.3925, -1.4036, -0.7279, -0.5594, -0.7688,  0.7624,  1.6423, -0.1596,\n",
      "         -0.4974,  0.4396],\n",
      "        [-0.7581,  1.0783,  0.8008,  1.6806,  1.2791,  1.2964,  0.6105,  1.3347,\n",
      "         -0.2316,  0.0418],\n",
      "        [-0.2516,  0.8599, -1.3847, -0.8712,  0.0780,  0.5258, -0.4880,  1.1914,\n",
      "         -0.8140, -0.7360],\n",
      "        [-0.8371, -0.9224, -0.0635,  0.6756, -0.0978,  1.8446, -1.1845,  1.3835,\n",
      "         -1.2024,  0.7078]], requires_grad=True) torch.Size([5, 10])\n",
      "Parameter containing:\n",
      "tensor([-0.5687,  1.2580, -1.5890, -1.1208,  0.8423], requires_grad=True) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for param in layer.parameters():\n",
    "    print(param, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3383,  1.6992,  0.0109, -0.3387, -1.3407, -0.5854,  0.5362,  0.5246,\n",
       "         -1.4692,  1.4332],\n",
       "        [ 0.7440, -0.4816, -1.0495,  0.6039, -1.7223, -0.8278, -0.4976,  0.4747,\n",
       "         -2.5095,  0.4880],\n",
       "        [ 0.7846,  0.0286,  0.6408,  0.5832,  0.2191,  0.5526, -0.1853,  0.7528,\n",
       "          0.4048,  0.1785],\n",
       "        [ 0.2649,  1.2732, -0.8905,  0.4098,  1.9312,  1.0119, -1.4364, -1.1299,\n",
       "         -0.1360,  1.6354],\n",
       "        [ 0.6547,  0.5760,  1.1415,  0.0186, -1.8058,  0.9254, -0.3753,  1.0331,\n",
       "         -0.6867,  0.6368]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(\n",
    "    size=(5,10),\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.7495,  1.6639, -1.6199,  0.6273,  0.6853],\n",
       "        [-0.2628,  3.3308, -4.8425,  1.1273,  4.3891],\n",
       "        [-1.4089, -0.1772,  1.1424, -1.8549,  2.4110],\n",
       "        [ 4.9050, -2.3195,  1.0556, -0.2721,  2.9166],\n",
       "        [ 0.0959,  1.2854, -0.2936, -0.5371,  4.7370]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = layer(data)\n",
    "print(output.shape)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Networks and complex blocks\n",
    "\n",
    "Now that we have our custom layer, we can now use it to build more complex structures, like blocks or even entire networks.\n",
    "\n",
    "Let's start by defining a small neural network using our previously defined custom layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = BasicLinear(\n",
    "            input_features=10, # expects input with shape (n, 10)\n",
    "            output_features=15\n",
    "        )\n",
    "        self.layer2 = BasicLinear(\n",
    "            input_features=15,\n",
    "            output_features=20 # should output with shape (n, 20)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        output_layer1 = self.layer1(x)\n",
    "        output_layer2 = self.layer2(output_layer1)\n",
    "        return output_layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.9269e+00,  1.4873e+00,  9.0072e-01, -2.1055e+00,  6.7842e-01,\n",
      "         -1.2345e+00, -4.3067e-02, -1.6047e+00, -7.5214e-01,  1.6487e+00],\n",
      "        [-3.9248e-01, -1.4036e+00, -7.2788e-01, -5.5943e-01, -7.6884e-01,\n",
      "          7.6245e-01,  1.6423e+00, -1.5960e-01, -4.9740e-01,  4.3959e-01],\n",
      "        [-7.5813e-01,  1.0783e+00,  8.0080e-01,  1.6806e+00,  1.2791e+00,\n",
      "          1.2964e+00,  6.1047e-01,  1.3347e+00, -2.3162e-01,  4.1759e-02],\n",
      "        [-2.5158e-01,  8.5986e-01, -1.3847e+00, -8.7124e-01, -2.2337e-01,\n",
      "          1.7174e+00,  3.1888e-01, -4.2452e-01,  3.0572e-01, -7.7459e-01],\n",
      "        [-1.5576e+00,  9.9564e-01, -8.7979e-01, -6.0114e-01, -1.2742e+00,\n",
      "          2.1228e+00, -1.2347e+00, -4.8791e-01, -9.1382e-01, -6.5814e-01],\n",
      "        [ 7.8024e-02,  5.2581e-01, -4.8799e-01,  1.1914e+00, -8.1401e-01,\n",
      "         -7.3599e-01, -1.4032e+00,  3.6004e-02, -6.3477e-02,  6.7561e-01],\n",
      "        [-9.7807e-02,  1.8446e+00, -1.1845e+00,  1.3835e+00,  1.4451e+00,\n",
      "          8.5641e-01,  2.2181e+00,  5.2317e-01,  3.4665e-01, -1.9733e-01],\n",
      "        [-1.0546e+00,  1.2780e+00, -1.7219e-01,  5.2379e-01,  5.6622e-02,\n",
      "          4.2630e-01,  5.7501e-01, -6.4172e-01, -2.2064e+00, -7.5080e-01],\n",
      "        [ 1.0868e-02, -3.3874e-01, -1.3407e+00, -5.8537e-01,  5.3619e-01,\n",
      "          5.2462e-01,  1.1412e+00,  5.1644e-02,  7.4395e-01, -4.8158e-01],\n",
      "        [-1.0495e+00,  6.0390e-01, -1.7223e+00, -8.2777e-01,  1.3347e+00,\n",
      "          4.8354e-01, -2.5095e+00,  4.8800e-01,  7.8459e-01,  2.8647e-02],\n",
      "        [ 6.4076e-01,  5.8325e-01,  1.0669e+00, -4.5015e-01, -1.8527e-01,\n",
      "          7.5276e-01,  4.0476e-01,  1.7847e-01,  2.6491e-01,  1.2732e+00],\n",
      "        [-1.3109e-03, -3.0360e-01, -1.4570e+00, -1.0234e-01, -5.9915e-01,\n",
      "          4.7706e-01,  7.2618e-01,  9.1152e-02, -3.8907e-01,  5.2792e-01],\n",
      "        [-1.2685e-02,  2.4084e-01,  1.3254e-01,  7.6424e-01,  1.0950e+00,\n",
      "          3.3989e-01,  7.1997e-01,  4.1141e-01,  1.9312e+00,  1.0119e+00],\n",
      "        [-1.4364e+00, -1.1299e+00, -1.3603e-01,  1.6354e+00, -7.3280e-01,\n",
      "          1.0430e-01,  1.0414e+00, -3.9973e-01, -2.2933e+00,  4.9756e-01],\n",
      "        [-4.2572e-01, -1.3371e+00, -1.1955e+00,  8.1234e-01, -3.0628e-01,\n",
      "         -3.3016e-01, -9.8080e-01,  1.9473e-01, -1.6535e+00,  6.8142e-01]],\n",
      "       requires_grad=True) torch.Size([15, 10])\n",
      "Parameter containing:\n",
      "tensor([ 1.0448,  1.8319,  0.7205, -0.1121, -0.0309, -0.1503,  1.8928,  1.3067,\n",
      "        -0.0662, -0.4235, -2.3768,  0.0641, -0.3435,  1.2287, -0.2754],\n",
      "       requires_grad=True) torch.Size([15])\n",
      "Parameter containing:\n",
      "tensor([[ 0.4788,  1.3537, -0.1593, -0.4249,  0.9442, -0.1849,  1.0608,  0.2083,\n",
      "          1.3065,  0.4598,  0.2618, -0.7599, -2.0461, -1.5295,  0.4049],\n",
      "        [ 0.6319, -0.1483, -2.3184,  1.3032,  0.4879,  1.1340, -0.3556,  0.3618,\n",
      "          1.9993,  1.0350,  1.6896,  0.0213, -0.8293, -1.0809, -0.7839],\n",
      "        [ 0.5071,  0.0821,  0.3999,  1.9892, -0.4611, -0.0639, -1.3667,  0.3298,\n",
      "         -0.9827,  0.3018,  0.1932,  0.4097, -1.5754,  2.2508,  1.0012],\n",
      "        [ 1.3642,  0.6333,  0.4050, -0.3532,  1.4639,  0.1729,  1.0514,  0.0075,\n",
      "         -0.0774,  0.6427,  0.5742,  0.5058,  0.2225, -0.9143,  1.4840],\n",
      "        [-0.9109, -0.5291, -0.8051,  0.5158, -0.1250,  0.7821,  0.5635,  1.8582,\n",
      "          1.0441, -0.8638,  0.8351, -0.3157, -1.9776,  0.0179, -1.4129],\n",
      "        [-1.8791, -0.1798,  0.7904,  0.5239, -0.2694,  1.7093,  0.0579,  0.8637,\n",
      "         -0.5890, -1.0340, -0.2179,  0.7987,  0.9105,  0.2691, -0.0366],\n",
      "        [-0.4808,  0.3163,  0.3866,  0.7337,  0.2510,  0.0770, -0.7150, -0.0476,\n",
      "          0.5230,  0.9717, -0.2779, -0.6116, -0.5572, -0.9683,  1.3433],\n",
      "        [ 0.7133,  0.3463, -0.5402,  0.8569, -0.6721,  1.0682, -0.2527, -2.3065,\n",
      "         -1.2869,  0.2137, -1.2351,  1.8592,  0.0561,  0.7694,  2.5574],\n",
      "        [ 1.2049, -0.9825,  0.3040,  0.9339, -1.9726, -1.4120,  1.7361,  1.8350,\n",
      "         -0.0047,  0.0795, -0.4560, -0.0619, -0.2222, -1.2470, -0.4862],\n",
      "        [-0.3360, -0.5871,  0.0827,  0.1858, -0.9698,  1.8932,  0.4447,  0.1364,\n",
      "          0.3088, -0.4875,  0.0501,  0.3273,  0.1292,  2.8520, -0.7436],\n",
      "        [ 0.1954, -1.3350, -0.5730, -0.3303, -0.3071, -0.7155,  0.0762, -0.2127,\n",
      "         -0.5663,  0.3989, -0.5513,  1.9890,  0.8479, -0.6953,  0.3056],\n",
      "        [ 0.2909,  0.4085, -1.2609, -0.3947,  1.8868,  0.1787, -0.0385, -0.0869,\n",
      "         -1.1803,  1.5460,  0.5448, -0.2040,  0.6854, -1.3351,  1.6516],\n",
      "        [ 1.9810, -0.1048,  0.4903, -0.4375, -0.4901, -0.3595, -0.0589, -0.4809,\n",
      "          0.9933,  0.2695, -1.8316,  0.3570, -0.6013, -0.0996, -1.2311],\n",
      "        [ 0.8657, -1.4236, -0.6961, -0.3182,  1.2154, -0.8076,  0.8244,  1.4862,\n",
      "         -1.4091, -0.3639, -0.0993,  0.3105,  0.3715,  0.1527, -0.0388],\n",
      "        [-0.2130, -0.5904,  0.4668,  0.3956, -2.7107, -0.6130, -0.1932,  0.7768,\n",
      "          0.7204, -1.3358,  0.8108,  0.7546,  0.4145,  1.3664, -0.0462],\n",
      "        [-1.6457, -0.6960,  0.2359,  0.8281,  0.1104, -0.3016, -1.4033, -0.5151,\n",
      "          0.6903,  2.0403, -1.2979,  0.6161, -0.2508,  0.8211, -0.0675],\n",
      "        [ 0.6686,  1.0430,  0.7204,  1.4701, -1.1215,  2.1004,  2.3664, -0.6323,\n",
      "          0.7607, -0.7463, -0.5824,  1.3500,  0.2778,  0.0916,  1.5164],\n",
      "        [-0.4389, -0.6421,  0.8266, -0.3646,  0.0872,  1.1225,  0.0980,  1.1226,\n",
      "          1.2491, -1.2620,  1.0861, -0.8786,  2.5487,  0.6594,  0.6702],\n",
      "        [-0.1260, -0.6126, -1.3062, -1.4040,  0.0953, -0.3659,  0.1961, -1.5174,\n",
      "          0.2059,  0.2722,  2.0487, -1.0880, -2.0479,  1.0006,  0.6495],\n",
      "        [ 0.0950, -0.7526, -0.6472, -1.2823,  1.9653, -1.1766,  1.1889,  0.7096,\n",
      "          0.8198,  0.6214,  0.4232, -0.3389,  0.5180, -1.7459,  1.0964]],\n",
      "       requires_grad=True) torch.Size([20, 15])\n",
      "Parameter containing:\n",
      "tensor([-0.6103,  0.1632,  1.5102,  0.2123, -1.4575, -0.9712,  0.2415, -1.1612,\n",
      "         0.1451,  1.6612,  0.5439, -0.5214,  1.2208, -0.6076, -0.0453, -0.3573,\n",
      "        -0.5714, -1.4078, -0.3764,  0.9874], requires_grad=True) torch.Size([20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (layer1): BasicLinear()\n",
       "  (layer2): BasicLinear()\n",
       ")"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNet()\n",
    "for param in model.parameters():\n",
    "    print(param, param.shape)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1678,  1.6433,  0.5163,  1.6060, -0.9815,  0.5361,  0.9226,  0.4872,\n",
       "         -0.9770, -0.0336],\n",
       "        [-0.7983, -0.2648, -0.1666,  0.2518,  1.2571,  1.2173,  0.3034, -0.4501,\n",
       "         -0.1739,  0.0299],\n",
       "        [-0.0140, -0.0102,  0.2337,  1.4083, -0.1743,  0.6092,  0.2254, -0.2793,\n",
       "          0.6702,  0.1188],\n",
       "        [-0.6119,  0.6026, -0.4403,  2.1848,  0.5258,  1.6828,  0.0967,  0.2571,\n",
       "          0.4728,  0.3640],\n",
       "        [-0.2812, -1.0375, -0.4976, -0.1823, -0.2120,  0.8162,  0.8982, -0.1539,\n",
       "         -0.5682, -0.0868]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(\n",
    "    size=(5,10),\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ -3.1472, -29.9071,   8.7836,   3.4756,  15.5517,  23.3254, -13.9781,\n",
       "          -16.4666,  11.5610,  26.2780, -12.2689, -20.9407,  -4.5172,  15.7965,\n",
       "           12.2754, -17.7336,  22.0269,  18.4722, -17.1346,  -5.1706],\n",
       "         [  4.1676, -14.8314,   3.4650,   6.1778,  -1.4382,   4.5206,   0.0622,\n",
       "           -9.7195,  10.3086,   6.7338,  -5.2735,  -7.4848,   3.2957,   2.4533,\n",
       "            1.2410,   4.0957,  13.7237,   4.0555, -17.8447,   4.4652],\n",
       "         [ -6.3661, -20.4782,  -3.6122,  -2.0613,  -1.2317,  14.0660,  -6.6215,\n",
       "           -4.6526,   2.7820,  14.8900,  -4.2800, -12.9097,  -0.8511,  -0.8365,\n",
       "            6.9943,  -4.1665,  15.3426,  11.1509, -13.1717,  -4.7658],\n",
       "         [ -5.4236, -28.9025,  -4.7860,   6.7843,  -1.0098,  27.0510,  -2.1414,\n",
       "          -10.6474,   5.8732,  19.9806,  -5.5848, -10.5738,  -9.7988,   1.8492,\n",
       "            2.7274,   5.9610,  28.6326,  21.9810, -26.0162,   4.0024],\n",
       "         [  5.9202, -11.4263,  14.1665,   0.8641,   3.0614,   4.1301,  -1.4829,\n",
       "            3.4788,  -2.9597,   9.9622,  -8.0175, -10.9831,   1.3440,  -3.1530,\n",
       "            6.8455,   1.7525,  15.6745,  -3.3727,  -9.1926,  -5.7175]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([5, 20]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(data)\n",
    "output, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! The output matches the expected shape\n",
    "\n",
    "Now let's try to implement a block and use it to build a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "class SimpleResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.fc1 = torch.nn.Linear(\n",
    "            in_features=in_features, out_features=out_features\n",
    "        )\n",
    "        self.fc2 = torch.nn.Linear(\n",
    "            in_features=out_features, out_features=out_features\n",
    "        )\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.downsample = None\n",
    "\n",
    "        # allows us to ensure the identity have the same shape\n",
    "        # as the output\n",
    "        if in_features != out_features:\n",
    "            self.downsample = torch.nn.Linear(\n",
    "                in_features=in_features, out_features=out_features\n",
    "            )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        identity = x\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "        out += identity\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleResidualBlock(\n",
       "  (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (downsample): Linear(in_features=10, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = SimpleResidualBlock(\n",
    "    in_features=10,\n",
    "    out_features=20\n",
    ")\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3382,  0.4742, -2.2940,  0.7744, -0.5453, -2.1582, -1.6608, -0.6637,\n",
       "         -0.2670,  0.2584],\n",
       "        [ 0.7758, -0.1000, -0.5615, -0.5949,  1.2687,  1.2904,  0.6930,  1.1980,\n",
       "          1.3964, -0.7150],\n",
       "        [ 1.4109, -1.3144, -1.3162, -1.2524, -1.6489, -0.2800, -1.2407,  0.7410,\n",
       "          0.7378, -0.8505],\n",
       "        [ 0.0361,  1.3407,  0.9860,  0.1132, -0.4233, -1.9508,  1.8619, -1.0779,\n",
       "          0.8849, -0.8342],\n",
       "        [ 1.0301, -0.8681,  0.2418,  1.3824,  1.1285, -1.2123,  2.6024, -0.0957,\n",
       "         -0.0811,  1.2587]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(\n",
    "    size=(5,10),\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4739, -1.9992,  1.0667, -1.4696,  0.4428, -0.8600,  0.2264, -1.3219,\n",
       "          0.2252,  0.6634,  0.0587,  0.2295,  0.3469,  0.8965,  0.0336,  1.0089,\n",
       "          1.1820, -0.3060,  0.1728,  0.5566],\n",
       "        [-0.7263, -0.6590,  0.3153, -1.6100,  0.0537, -0.3231, -0.4889,  0.3054,\n",
       "         -0.8394,  0.5122, -1.0412, -0.0720, -1.4181,  0.3687, -1.0107, -0.8467,\n",
       "          0.2860, -0.1833,  0.4961,  0.4846],\n",
       "        [ 0.5692, -1.1106, -0.5407, -1.6185,  1.0318, -1.6888,  0.5215, -1.2556,\n",
       "         -1.0459,  0.9259, -1.0388, -0.6898, -0.3632,  1.4558, -0.7626, -0.7203,\n",
       "          0.7005, -1.4810,  0.3062,  1.5153],\n",
       "        [ 0.3583, -1.3263, -0.3940, -1.0462, -1.2971, -0.6285, -0.0323,  0.1265,\n",
       "          0.2430, -1.2454,  0.8915, -0.8012,  0.0533,  0.3077, -0.1811,  0.4346,\n",
       "          0.8679, -0.7155,  1.0592, -0.4466],\n",
       "        [ 0.2313, -0.1927, -0.9310, -1.5572, -1.4468,  0.2535,  0.6084, -0.5691,\n",
       "         -0.3841,  0.3957,  1.6790,  1.0765, -0.0834, -0.4265,  0.9681,  0.5116,\n",
       "          0.3979,  0.3903, -0.2494, -0.6807]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = block(data)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our block, let's use it to build a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.block1 = SimpleResidualBlock(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features\n",
    "        )\n",
    "        self.block2 = SimpleResidualBlock(\n",
    "            in_features=out_features,\n",
    "            out_features=out_features\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (block1): SimpleResidualBlock(\n",
       "    (fc1): Linear(in_features=10, out_features=50, bias=True)\n",
       "    (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (downsample): Linear(in_features=10, out_features=50, bias=True)\n",
       "  )\n",
       "  (block2): SimpleResidualBlock(\n",
       "    (fc1): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNet(\n",
    "    in_features=10,\n",
    "    out_features=50\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2182,  0.8823,  0.5390,  1.3357,  0.8349, -1.0390, -0.4415, -0.4136,\n",
       "          0.6149,  0.5247],\n",
       "        [ 0.1156,  0.9289, -1.1753,  1.4462,  0.2890, -0.5746,  0.4203,  0.3187,\n",
       "         -0.1949, -0.7710],\n",
       "        [-1.0754, -0.6555, -0.5378, -0.3390,  1.3493, -1.5745, -0.5291,  2.2761,\n",
       "          0.2758,  0.4236],\n",
       "        [-1.7807, -0.2473,  1.3181,  1.8177,  1.5550,  1.1142, -0.2878, -1.0536,\n",
       "         -1.5974, -0.1525],\n",
       "        [ 0.2308,  1.0065,  0.1740,  1.5454, -0.8084,  1.7691,  0.1786,  0.5163,\n",
       "         -0.4629, -0.6336]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(\n",
    "    size=(5,10),\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(data)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we can use and add any class that inherits from the `torch.nn.Module` as lego blocks for larger blocks or networks. This allows for a lot of flexibility when designing networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "In this section we will explore some of the layers available in the pytorch ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "This is one of the most basic layers, which is a fully connected layer that applies a linear transformation to the input. It is used, for example, as the final layer of Convolutional Neural Networks to output the classes probabilities.\n",
    "\n",
    "The output is given by:\n",
    "$$ y = x\\cdot{A^T} + b $$\n",
    "\n",
    "Where:\n",
    "- $x$ is the input of the layer\n",
    "- $A$ is the weights matrix, which in this case is transposed ($A^T$)\n",
    "- $b$ is the bias term\n",
    "\n",
    "In other words, the linear layer is a dot product of the input tensor and the weights tensor, plus the bias term to offset the weights and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7910, -0.6975,  0.4384,  0.7299,  1.0319],\n",
       "        [-0.2977,  0.5749, -0.3397,  0.9044,  1.0887],\n",
       "        [ 0.3056, -0.6722, -0.0591,  0.5983, -0.2779],\n",
       "        [ 0.1036, -0.0570,  0.0212,  0.9951,  0.7813],\n",
       "        [ 0.5157,  0.1053, -1.0661,  1.8080,  0.9811]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "linear = torch.nn.Linear(in_features=10, # in_features = matches inner dimension of input \n",
    "                         out_features=5) # out_features = describes outer value of output\n",
    "# this layer expects a (n, 10) input and will output a (n, 5). See below.\n",
    "\n",
    "x = torch.randn(\n",
    "    size=(5,10),\n",
    ")\n",
    "output = linear(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2418,  0.2625, -0.0741,  0.2905, -0.0693,  0.0638, -0.1540,  0.1857,\n",
      "          0.2788, -0.2320],\n",
      "        [ 0.2749,  0.0592,  0.2336,  0.0428,  0.1525, -0.0446,  0.2438,  0.0467,\n",
      "         -0.1476,  0.0806],\n",
      "        [-0.1457, -0.0371, -0.1284,  0.2098, -0.2496, -0.1458, -0.0893, -0.1901,\n",
      "          0.0298, -0.3123],\n",
      "        [ 0.2856, -0.2686,  0.2441,  0.0526, -0.1027,  0.1954,  0.0493,  0.2555,\n",
      "          0.0346, -0.0997],\n",
      "        [ 0.0850, -0.0858,  0.1331,  0.2823,  0.1828, -0.1382,  0.1825,  0.0566,\n",
      "          0.1606, -0.1927]], requires_grad=True) torch.Size([5, 10])\n",
      "Parameter containing:\n",
      "tensor([-0.3130, -0.1222, -0.2426,  0.2595,  0.0911], requires_grad=True) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for param in linear.parameters():\n",
    "    print(param, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with different dimensions this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4960, -0.2588,  0.8303],\n",
       "        [ 0.1278, -0.1437,  0.2210],\n",
       "        [-0.5078, -0.1890, -0.4775],\n",
       "        [-1.1359, -0.5379, -0.2967],\n",
       "        [-0.4981, -0.2988, -0.2436],\n",
       "        [ 0.3976,  0.0798,  0.2285],\n",
       "        [-0.1890, -0.3357,  0.3579],\n",
       "        [-0.3978, -0.8552,  0.5626],\n",
       "        [-0.0656, -0.3253,  0.0333]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "linear = torch.nn.Linear(in_features=7, # in_features = matches inner dimension of input \n",
    "                         out_features=3) # out_features = describes outer value \n",
    "x = torch.randn(\n",
    "    size=(9,7),\n",
    ")\n",
    "output = linear(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 3])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2890,  0.3137, -0.0885,  0.3472, -0.0828,  0.0763, -0.1840],\n",
      "        [ 0.2220,  0.3332, -0.2773,  0.3285,  0.0707,  0.2792,  0.0512],\n",
      "        [ 0.1822, -0.0534,  0.2914,  0.0559, -0.1764,  0.0963, -0.1741]],\n",
      "       requires_grad=True) torch.Size([3, 7])\n",
      "Parameter containing:\n",
      "tensor([-0.0443, -0.1535,  0.2507], requires_grad=True) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for param in linear.parameters():\n",
    "    print(param, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapes\n",
    "\n",
    "Let's try to understand the shapes we defined.\n",
    "\n",
    "When creating the layer, we defined 2 parameters:\n",
    "- in_features\n",
    "- out_features\n",
    "\n",
    "The figure below explains how the shapes of the inputs and outputs are obtained\n",
    "\n",
    "<img src=\"../assets/layers/linear layer.png\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilinear\n",
    "\n",
    "The bilinear layer has several advantages over traditional linear layers. First, it can capture more complex relationships between pairs of input features compared to linear layers, which only consider their linear combinations. Second, it can reduce the dimensionality of the output space by combining multiple features into a single vector, making it more efficient for downstream tasks.\n",
    "\n",
    "The output is given by:\n",
    "$$ y=x_{1}^{T}A x_{2}+b $$\n",
    "\n",
    "Where:\n",
    "- $x_1$ is the input tensor 1 of the layer, in this case transposed $(x_1^T)$\n",
    "- $x_2$ is the input tensor 2 of the layer\n",
    "- $A$ is the weights matrix, which in this case is transposed ($A^T$)\n",
    "- $b$ is the bias term\n",
    "\n",
    "One thing to note is that *matrix dot products are associative* which means that\n",
    "\n",
    "$$(x_{1}^{T}A) x_{2} = x_{1}^{T} (Ax_{2})$$\n",
    "\n",
    "In other words, the linear layer is a dot product of the input tensor and the weights tensor, plus the bias term to offset the weights and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-3.7503e+00, -2.5997e+00,  1.5886e+00, -1.6809e+00, -1.7854e+00,\n",
       "           1.2302e+00, -2.1120e+00,  8.2458e-01, -1.4203e+00, -2.5396e-01,\n",
       "           7.4477e-01, -1.6202e+00,  7.1375e+00,  7.7517e-01,  3.8935e+00,\n",
       "          -1.3744e+00,  2.1667e+00, -1.1615e+00, -5.5572e+00, -3.1567e+00,\n",
       "          -8.3902e-02,  3.3506e+00, -4.8061e+00,  4.3645e-01,  7.5092e-01,\n",
       "           5.5137e+00,  3.2257e+00,  1.7349e+00,  3.6345e-01, -9.0051e-01,\n",
       "          -2.9298e+00, -2.6614e+00, -6.7511e-01,  1.1245e+00,  1.2597e+00,\n",
       "           4.9732e-01,  1.5067e+00, -2.5743e-01, -1.6297e+00,  3.6605e+00,\n",
       "           2.5494e+00,  1.8083e+00,  2.0576e+00,  9.2959e-01, -2.2974e+00,\n",
       "          -2.8003e+00, -3.1339e-01,  1.9601e+00,  1.6979e+00,  1.8401e+00],\n",
       "         [ 1.7927e+00,  1.7303e+00,  2.2798e+00,  8.4715e-01, -2.5555e+00,\n",
       "           3.5886e+00, -4.5120e+00, -2.3588e+00, -7.3369e-01,  8.1743e-01,\n",
       "          -1.9012e+00, -1.5614e+00,  1.6590e-01,  1.7384e+00, -2.5659e-01,\n",
       "          -1.4034e+00, -9.3821e-01, -5.2841e+00, -1.2746e+00, -3.0108e+00,\n",
       "          -9.4563e-01,  3.9188e+00,  4.2019e-01,  6.8507e-01, -1.9361e+00,\n",
       "           3.5185e-01, -6.2301e-01, -2.3331e+00, -2.4804e+00, -6.5360e-01,\n",
       "          -3.4320e+00, -2.0575e+00, -3.7934e-01,  1.3889e+00,  3.5562e+00,\n",
       "          -3.9184e+00, -1.2288e+00,  7.6122e-01, -2.8187e+00,  2.6964e+00,\n",
       "           8.5467e-01,  1.1448e+00, -1.8206e+00, -1.8414e-01,  3.1583e+00,\n",
       "           5.1255e-01,  1.9299e+00,  2.4057e+00,  1.4874e+00,  2.9593e+00],\n",
       "         [-5.7767e-01,  3.9234e+00, -7.3252e-01, -3.0437e+00,  8.0261e-01,\n",
       "           1.6576e+00,  3.7567e+00, -2.1591e+00,  2.5737e+00,  2.9534e+00,\n",
       "          -8.7223e-01,  3.9714e+00, -1.3315e+00, -7.2727e+00, -1.5282e-02,\n",
       "          -8.1189e-01,  3.1887e-02, -2.2793e+00,  2.9230e+00,  8.5366e-01,\n",
       "          -2.9975e+00, -9.0562e-01,  3.6775e+00,  1.4605e+00, -5.1616e-01,\n",
       "          -3.3196e-01, -2.6539e+00, -7.1059e+00, -4.9499e+00, -2.1087e+00,\n",
       "           5.8196e-01,  1.8939e+00, -1.3357e-01, -7.3056e-01,  5.0906e+00,\n",
       "           5.4488e-01, -3.7585e+00,  3.3300e+00, -7.3971e-01,  1.5334e-03,\n",
       "           3.0425e+00, -8.5377e-01, -2.8989e+00, -1.3598e+00, -6.4893e-01,\n",
       "          -5.2271e-02, -7.8114e-02,  1.0895e+00,  7.8991e-01,  3.1725e+00],\n",
       "         [-7.3929e-02, -1.0854e+00,  1.6148e+00, -9.2589e-01,  2.9618e+00,\n",
       "          -6.6230e-01, -5.6116e+00, -1.3421e+00, -4.4953e-01, -4.5361e+00,\n",
       "          -2.6325e+00,  1.5607e+00, -1.2098e+00, -6.3961e+00, -3.8681e+00,\n",
       "           2.3651e+00,  6.7349e-02, -7.5257e+00,  9.9472e-01, -8.3009e-01,\n",
       "           8.9740e-01, -1.7561e+00,  4.4983e+00,  3.7524e+00, -2.0504e+00,\n",
       "           4.4287e+00,  3.6173e+00,  7.2921e-01,  4.0958e+00,  3.4583e+00,\n",
       "          -1.5102e+00, -1.0927e+00, -4.6862e+00, -4.7927e+00, -3.4434e+00,\n",
       "           1.0496e-01, -1.1145e+00, -3.0896e+00,  4.7526e+00, -6.3717e+00,\n",
       "           1.4364e+00,  3.1809e+00, -3.2491e+00, -4.5543e-01,  6.1273e+00,\n",
       "           1.4427e+00,  8.3014e-02,  3.1755e-03, -2.4658e+00,  4.8616e+00],\n",
       "         [-3.8665e+00, -1.2922e+00, -2.0581e+00, -4.5659e-01, -1.0514e+00,\n",
       "           1.4497e+00, -3.8841e+00,  1.1799e+00, -4.3201e-01,  2.6366e+00,\n",
       "          -6.5121e-01, -1.7204e+00,  4.8861e+00,  1.8291e+00,  1.9548e+00,\n",
       "           2.1939e+00, -3.3747e+00, -1.1722e+00, -1.7820e+00,  2.5409e+00,\n",
       "          -2.9543e+00,  1.3120e+00, -1.2825e+00, -3.3504e+00, -3.0555e+00,\n",
       "           1.7011e-01,  8.8045e-01, -3.8021e-01, -2.7740e+00,  3.4849e+00,\n",
       "          -2.6087e+00, -1.6228e+00, -1.2931e-01,  1.6583e+00,  2.6383e-01,\n",
       "           1.6965e+00,  5.1345e+00,  8.3810e-01,  9.6648e-01,  7.5102e-01,\n",
       "          -9.9047e-02,  7.5447e-01, -3.1588e+00, -7.4520e-01,  3.5357e+00,\n",
       "           2.0138e+00,  4.7734e+00, -1.2977e-01,  8.4979e-01, -2.6717e-02]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([5, 50]))"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "bilinear = torch.nn.Bilinear(\n",
    "    in1_features=10, # in_features = matches inner dimension of input \n",
    "    in2_features=20,\n",
    "    out_features=50 # out_features = describes outer value of output\n",
    ")\n",
    "# this layer expects a (n, 10) input and will output a (n, 5). See below.\n",
    "\n",
    "x1 = torch.randn(\n",
    "    size=(5,10),\n",
    ")\n",
    "x2 = torch.randn(\n",
    "    size=(5,20),\n",
    ")\n",
    "output = bilinear(x1, x2)\n",
    "output, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1d\n",
    "\n",
    "This is one of the most basic layers, which is a fully connected layer that applies a linear transformation to the input. It is used, for example, as the final layer of Convolutional Neural Networks to output the classes probabilities.\n",
    "\n",
    "The output is given by:\n",
    "$$ y = x\\cdot{A^T} + b $$\n",
    "\n",
    "Where:\n",
    "- $x$ is the input of the layer\n",
    "- $A$ is the weights matrix, which in this case is transposed ($A^T$)\n",
    "- $b$ is the bias term\n",
    "\n",
    "In other words, the linear layer is a dot product of the input tensor and the weights tensor, plus the bias term to offset the weights and inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
