{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Layers\n",
    "\n",
    "PyTorch is build around torch.nn, which have classes like `torch.nn.Module`, `torch.nn.Parameter`. Think of these as basic building blocks, or Lego blocks, that allow you to build complex structures. Layers, Models inherit from `torch.nn.Module`, which already defines a lot of useful methods and allows us to build our own blocks with as little as defining a `__init__()` and a `forward()` methods.\n",
    "\n",
    "For example, we can create a new layer with something as simple as:\n",
    "```python\n",
    "import torch\n",
    "class BasicLinear(torch.nn.Module): # Inherits from nn.Module. Almost everything in PyTorch is a nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = torch.nn.Parameter( # defines weights Parameter\n",
    "            data=torch.randn(1), # starts with random value\n",
    "            requires_grad=True # activate autograd, which allows us to track and update this parameter during backward pass\n",
    "        )\n",
    "        self.bias = torch.nn.Parameter( # defines bias Parameter, same as above but with different syntax\n",
    "            torch.randn(1, dtype=torch.float32), # explicitly sets the dtype to float32 (the default)\n",
    "            requires_grad=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor): # defines the way data will be transformed in the layer or block\n",
    "        # linear = xA^T + b\n",
    "        # A^T\n",
    "        weights = self.weights.t() # transposes weights tensor\n",
    "        # x\n",
    "        input_tensor = x\n",
    "        # b\n",
    "        bias = self.bias\n",
    "\n",
    "        # xA^T\n",
    "        mul = torch.matmul(input_tensor, weights) # calculates tensor dot product between transposed weights and input\n",
    "\n",
    "        # xA^T + b\n",
    "        output = mul + bias # adds offset (bias)\n",
    "\n",
    "        return output\n",
    "```\n",
    "In the forward method we follow the [implementation of the `nn.functional.linear` layer](https://pytorch.org/docs/stable/generated/torch.nn.functional.linear.html), which is implemented in C++. The [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) uses the ``nn.functional.linear` layer (written in C++) in its forward method. But at the end of the day, the transformation being made is:\n",
    "\n",
    "$$y=x A^{T}+b.$$\n",
    "\n",
    "In this notebook we are gonna see how we can create our own layers, and explore some of the predefined layers included with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
      "Requirement already satisfied: numpy==1.25.2 in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
      "Requirement already satisfied: pillow==9.4.0 in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
      "Requirement already satisfied: torchvision==0.18 in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
      "Torch version: 2.3.0+cu121\n",
      "Numpy version: 1.25.2\n",
      "PIL version: 9.4.0\n",
      "GPU enabled: True\n"
     ]
    }
   ],
   "source": [
    "# Ensures versions are correct\n",
    "! pip install torch==2.3.0 numpy==1.25.2 pillow==9.4.0 torchvision==0.18\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"PIL version: {PIL.__version__}\")\n",
    "print(f\"GPU enabled: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Building Blocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layers\n",
    "\n",
    "You can build your own layers using class definitions that inherit from torch.nn.Module. These can be used as building blocks for larger networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "class BasicLinear(torch.nn.Module): # Inherits from nn.Module. Almost everything in PyTorch is a nn.Module\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super().__init__()\n",
    "        self.weights = torch.nn.Parameter( # defines weights Parameter\n",
    "            data=torch.randn(size=(output_features, input_features)), # starts with random value\n",
    "            requires_grad=True # activate autograd, which allows us to track and update this parameter during backward pass\n",
    "        )\n",
    "        self.bias = torch.nn.Parameter( # defines bias Parameter, same as above but with different syntax\n",
    "            torch.randn(size=(output_features,), dtype=torch.float32), # explicitly sets the dtype to float32 (the default)\n",
    "            requires_grad=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # defines the way data will be transformed in the layer or block\n",
    "        # linear = xA^T + b\n",
    "        # A^T\n",
    "        weights = self.weights.t() # transposes weights tensor\n",
    "        # x\n",
    "        input_tensor = x\n",
    "        # b\n",
    "        bias = self.bias\n",
    "\n",
    "        # xA^T\n",
    "        mul = torch.matmul(input_tensor, weights) # calculates tensor dot product between transposed weights and input\n",
    "\n",
    "        # xA^T + b\n",
    "        output = mul + bias # adds offset (bias)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry about the dimensions yet, we will understand it more later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicLinear()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = BasicLinear(input_features=10, output_features=5)\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047,\n",
      "         -0.7521,  1.6487],\n",
      "        [-0.3925, -1.4036, -0.7279, -0.5594, -0.7688,  0.7624,  1.6423, -0.1596,\n",
      "         -0.4974,  0.4396],\n",
      "        [-0.7581,  1.0783,  0.8008,  1.6806,  1.2791,  1.2964,  0.6105,  1.3347,\n",
      "         -0.2316,  0.0418],\n",
      "        [-0.2516,  0.8599, -1.3847, -0.8712,  0.0780,  0.5258, -0.4880,  1.1914,\n",
      "         -0.8140, -0.7360],\n",
      "        [-0.8371, -0.9224, -0.0635,  0.6756, -0.0978,  1.8446, -1.1845,  1.3835,\n",
      "         -1.2024,  0.7078]], requires_grad=True) torch.Size([5, 10])\n",
      "Parameter containing:\n",
      "tensor([-0.5687,  1.2580, -1.5890, -1.1208,  0.8423], requires_grad=True) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for param in layer.parameters():\n",
    "    print(param, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3383,  1.6992,  0.0109, -0.3387, -1.3407, -0.5854,  0.5362,  0.5246,\n",
       "         -1.4692,  1.4332],\n",
       "        [ 0.7440, -0.4816, -1.0495,  0.6039, -1.7223, -0.8278, -0.4976,  0.4747,\n",
       "         -2.5095,  0.4880],\n",
       "        [ 0.7846,  0.0286,  0.6408,  0.5832,  0.2191,  0.5526, -0.1853,  0.7528,\n",
       "          0.4048,  0.1785],\n",
       "        [ 0.2649,  1.2732, -0.8905,  0.4098,  1.9312,  1.0119, -1.4364, -1.1299,\n",
       "         -0.1360,  1.6354],\n",
       "        [ 0.6547,  0.5760,  1.1415,  0.0186, -1.8058,  0.9254, -0.3753,  1.0331,\n",
       "         -0.6867,  0.6368]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(\n",
    "    size=(5,10),\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.7495,  1.6639, -1.6199,  0.6273,  0.6853],\n",
       "        [-0.2628,  3.3308, -4.8425,  1.1273,  4.3891],\n",
       "        [-1.4089, -0.1772,  1.1424, -1.8549,  2.4110],\n",
       "        [ 4.9050, -2.3195,  1.0556, -0.2721,  2.9166],\n",
       "        [ 0.0959,  1.2854, -0.2936, -0.5371,  4.7370]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = layer(data)\n",
    "print(output.shape)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "The output is given by:\n",
    "$$ y = x\\cdot{A^T} + b $$\n",
    "\n",
    "Where:\n",
    "- $x$ is the input of the layer\n",
    "- $A$ is the weights matrix, which in this case is transposed ($A^T$)\n",
    "- $b$ is the bias term\n",
    "\n",
    "In other words, the linear layer is a dot product of the input tensor and the weights tensor, plus the bias term to offset the weights and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7910, -0.6975,  0.4384,  0.7299,  1.0319],\n",
       "        [-0.2977,  0.5749, -0.3397,  0.9044,  1.0887],\n",
       "        [ 0.3056, -0.6722, -0.0591,  0.5983, -0.2779],\n",
       "        [ 0.1036, -0.0570,  0.0212,  0.9951,  0.7813],\n",
       "        [ 0.5157,  0.1053, -1.0661,  1.8080,  0.9811]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "linear = torch.nn.Linear(in_features=10, # in_features = matches inner dimension of input \n",
    "                         out_features=5) # out_features = describes outer value \n",
    "x = torch.randn(\n",
    "    size=(5,10),\n",
    ")\n",
    "output = linear(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2418,  0.2625, -0.0741,  0.2905, -0.0693,  0.0638, -0.1540,  0.1857,\n",
      "          0.2788, -0.2320],\n",
      "        [ 0.2749,  0.0592,  0.2336,  0.0428,  0.1525, -0.0446,  0.2438,  0.0467,\n",
      "         -0.1476,  0.0806],\n",
      "        [-0.1457, -0.0371, -0.1284,  0.2098, -0.2496, -0.1458, -0.0893, -0.1901,\n",
      "          0.0298, -0.3123],\n",
      "        [ 0.2856, -0.2686,  0.2441,  0.0526, -0.1027,  0.1954,  0.0493,  0.2555,\n",
      "          0.0346, -0.0997],\n",
      "        [ 0.0850, -0.0858,  0.1331,  0.2823,  0.1828, -0.1382,  0.1825,  0.0566,\n",
      "          0.1606, -0.1927]], requires_grad=True) torch.Size([5, 10])\n",
      "Parameter containing:\n",
      "tensor([-0.3130, -0.1222, -0.2426,  0.2595,  0.0911], requires_grad=True) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for param in linear.parameters():\n",
    "    print(param, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with different dimensions this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4960, -0.2588,  0.8303],\n",
       "        [ 0.1278, -0.1437,  0.2210],\n",
       "        [-0.5078, -0.1890, -0.4775],\n",
       "        [-1.1359, -0.5379, -0.2967],\n",
       "        [-0.4981, -0.2988, -0.2436],\n",
       "        [ 0.3976,  0.0798,  0.2285],\n",
       "        [-0.1890, -0.3357,  0.3579],\n",
       "        [-0.3978, -0.8552,  0.5626],\n",
       "        [-0.0656, -0.3253,  0.0333]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "linear = torch.nn.Linear(in_features=7, # in_features = matches inner dimension of input \n",
    "                         out_features=3) # out_features = describes outer value \n",
    "x = torch.randn(\n",
    "    size=(9,7),\n",
    ")\n",
    "output = linear(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2890,  0.3137, -0.0885,  0.3472, -0.0828,  0.0763, -0.1840],\n",
      "        [ 0.2220,  0.3332, -0.2773,  0.3285,  0.0707,  0.2792,  0.0512],\n",
      "        [ 0.1822, -0.0534,  0.2914,  0.0559, -0.1764,  0.0963, -0.1741]],\n",
      "       requires_grad=True) torch.Size([3, 7])\n",
      "Parameter containing:\n",
      "tensor([-0.0443, -0.1535,  0.2507], requires_grad=True) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for param in linear.parameters():\n",
    "    print(param, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapes\n",
    "\n",
    "Let's try to understand the shapes we defined.\n",
    "\n",
    "When creating the layer, we defined 2 parameters:\n",
    "- in_features\n",
    "- out_features\n",
    "\n",
    "The figure below explains how the shapes of the inputs and outputs are obtained\n",
    "\n",
    "<img src=\"../assets/layers/linear layer.png\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
